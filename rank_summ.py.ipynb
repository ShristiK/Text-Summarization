{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization using Senetnce Ranking\n",
    "### STEP 1 : Data cleaning ( removing non letter characters, turning to lower case letters )\n",
    "### STEP 2 : Building Sentence Similarity Matrix\n",
    "### STEP 3 : Sentence Ranking\n",
    "### STEP 4 : Summary Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Phase\n",
    "### Importing Libraries and Reading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing the necessary libraries\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the data file\n",
    "\n",
    "df = pandas.read_csv('Downloads/tennis_articles_v4.csv')\n",
    "# df['article_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "### working with re ( regular expression in python)\n",
    "\n",
    "# import re\n",
    "# s = 'he&&&s'\n",
    "# s = re.sub(\"[^a-zA-Z]\",\" \",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : Data Cleaning\n",
    "### Cleaning sentences, by removing Non Alphabet Characters and converting to Lower Case Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning sentences, by removing non alphabet characters and converting to lower case letters\n",
    "\n",
    "dict = {}\n",
    "s = \"\"\n",
    "for a in df['article_text']:\n",
    "      s += a\n",
    "# print s\n",
    "s = s.lower()\n",
    "# print s\n",
    "\n",
    "sentences = sent_tokenize(s)\n",
    "# print sentences\n",
    "\n",
    "final = []\n",
    "\n",
    "for s in sentences:\n",
    "      temp = re.sub(\"[^a-zA-Z]\",\" \",s)\n",
    "      temp = temp.lower()\n",
    "      final.append(temp)\n",
    "      dict[temp] = s\n",
    "# printfinal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : Building Senetnce Similarity Matrix\n",
    "### Similarity is found using Cosine Similarity between vector representation of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define method for calculating similarity\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 : Sentence Ranking\n",
    "### Sentences are ranked using PageRank Algorithm on the Graph generated from the Sentence Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generating the final summary : graph is generated using networkx library and cosine similarity matrix \n",
    "# containg adjacency list; after that sentences are scored using pagerank and sorted and stored in ranked_sentences\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "sentence_similarity_martix = build_similarity_matrix(final, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(final)), reverse=True)    \n",
    "# print type(ranked_sentence)\n",
    "# print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "\n",
    "\n",
    "# Step 5 - Offcourse, output the summarize texr\n",
    "# print('Summarize Text: \\n', \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 : Summary Generation\n",
    "### Summary is outputted as the top 10 ranked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaking at the swiss indoors tournament where he will play in sundays final against romanian qualifier marius copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment.\n",
      "the 20-time grand slam champion has voiced doubts about the wisdom of the one-week format to be introduced by organisers kosmos, who have promised the international tennis federation up to $3 billion in prize money over the next quarter-century.\n",
      "argentina and britain received wild cards to the new-look event, and will compete along with the four 2018 semi-finalists and the 12 teams who win qualifying rounds next february.\n",
      "when i'm on the courts or when i'm on the court playing, i'm a competitor and i want to beat every single person whether they're in the locker room or across the net.so i'm not the one to strike up a conversation about the weather and know that in the next few minutes i have to go and try to win a tennis match.\n",
      "but with the atp world tour finals due to begin next month, nadal is ready to prove his fitness before the season-ending event at the 02 arena.\n",
      "davenport enjoyed most of her success in the late 1990s and her third and final major tournament win came at the 2000 australian open.\n",
      "\"not always, but i really feel like in the mid-2000 years there was a huge shift of the attitudes of the top players and being more friendly and being more giving, and a lot of that had to do with players like roger coming up.\n",
      "seeking a ninth title at his hometown event, and a 99th overall, federer will play 93th-ranked marius copil on sunday.\n",
      "the russian player has no problems in openly speaking about it and in a recent interview she said: 'i don't really hide any feelings too much.\n",
      "federer is in action at the swiss indoors in basel and if he reaches the final, he could pull out of paris in a bid to stay fresh for london.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "     print dict[ranked_sentence[i][1]]\n",
    "# for i in range(10):\n",
    "#       summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
